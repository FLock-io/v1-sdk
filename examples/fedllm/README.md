## Installation 

The code requires some dependencies (Python=3.8)  as specified in `requirements.txt`. Please follow the relevant libraries to install or run:
```bash
pip install -r requirements.txt
```
If `bitsandbytes` doesn't work, [install it from source](https://github.com/TimDettmers/bitsandbytes/blob/main/compile_from_source.md). Windows users can follow [these instructions](https://github.com/tloen/alpaca-lora/issues/17).

## Model Preparation
[ref](https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md#how-to-apply-delta-weights-for-weights-v11-and-v0)

Install git lfs
```bash
sudo apt-get install git-lfs
```

Install fschat
```bash
pip3 install fschat
````

Download LLaMA
```bash
git lfs clone https://huggingface.co/decapoda-research/llama-7b-hf
# Or
git lfs clone https://huggingface.co/yahma/llama-7b-hf
```

Download the Vicuna model weights
```bash
git lfs clone https://huggingface.co/lmsys/vicuna-7b-delta-v1.1
```

Mix the LLaMA and Vicuna model weights
```bash
python3 -m fastchat.model.apply_delta \
    --base /model/llama-7b-hf \
    --delta /model/vicuna-7b-delta-v1.1 \
    --target /model/vicuna-7b-all-v1.1 
```
## Data_Preparation

Prior to commencing the federated fine-tuning, make sure to create a data file for each individual client.
```bash
num_client=10 # The number of clients
diff_quantity=0 # Whether clients have different amounts of data
python client_data_allocation.py $num_client $diff_quantity
```
Running this command will save the data files in the folder `./data/str(num_client)`. The data file `new-databricks-dolly-15k.json` for generating each client's local dataset is the first version of `databricks-dolly-15k` , which is a corpus of more than 15,000 records with 8 categeries generated by thousands of [Databricks Lab](https://www.databricks.com/learn/labs) employees. Please refer to their official repository [dolly](https://github.com/databrickslabs/dolly) for the latest version of data.

### Categories distribution and Heteogeneity
The first version of `databricks-dolly-15k` contains 8 Categories, with the distribution of each category shown in the following subfigure provided on the right.

<p align="center">
  <img src="assets/twodonuts.png" width="150%">
</p>

Without federated learning, the model can be trained on only the particular local instruction categories of each user (left) due to privacy or cost issue. By implementing our Federated instruction tuning ([***FedIT***](https://arxiv.org/pdf/2305.05644.pdf)) framework with this repo *Shepherd*, the LLM can be trained on the local instruction datasets of all clients with greater diversity and quantity of data points that cover the entire range of the subject matter (right).

The following figure presents an illustrative depiction of the category distributions among each client, serving to exemplify the heterogeneity nature of clients' instructions.

<p align="center">
  <img src="assets/hetero.png" width="150%">
</p>

### Use your own data

You can simply modify `client_data_allocation.py` to load your own  dataset for federated training.



