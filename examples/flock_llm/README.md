## Overview
Prepared dataset and model weights could be found via https://drive.google.com/file/d/1ehdGXhKGlIOf8c8hit8QjM1JebCmWrai/view?usp=sharing

Pls directly unzip the file to the root directory of this project.

## Installation 

The code requires some dependencies (Python=3.8)  as specified in `requirements.txt`. Please follow the relevant libraries to install or run:
```bash
pip install -r requirements.txt
```
If `bitsandbytes` doesn't work, [install it from source](https://github.com/TimDettmers/bitsandbytes/blob/main/compile_from_source.md). Windows users can follow [these instructions](https://github.com/tloen/alpaca-lora/issues/17).

## Model Preparation
[ref](https://github.com/lm-sys/FastChat/blob/main/docs/vicuna_weights_version.md#how-to-apply-delta-weights-for-weights-v11-and-v0)

Install git lfs
```bash
sudo apt-get install git-lfs
```

Install fschat
```bash
pip3 install fschat
````

Download LLaMA
```bash
git lfs clone https://huggingface.co/decapoda-research/llama-7b-hf
# Or
git lfs clone https://huggingface.co/yahma/llama-7b-hf
```

Download the Vicuna model weights
```bash
git lfs clone https://huggingface.co/lmsys/vicuna-7b-delta-v1.1
```

Mix the LLaMA and Vicuna model weights
```bash
python3 -m fastchat.model.apply_delta \
    --base ./model/llama-7b-hf \
    --delta ./model/vicuna-7b-delta-v1.1 \
    --target ./model/vicuna-7b-all-v1.1 
```
Note: the vicuna-7b-all-v1.1 model is used as the initial model for federated fine-tuning, and takes 13GB disk storage space.

## Data_Preparation

Prior to commencing the federated fine-tuning, make sure to create a data file for each individual client.
```bash
num_client=10 # The number of clients
diff_quantity=0 # Whether clients have different amounts of data
python client_data_allocation.py $num_client $diff_quantity
```
Running this command will save the data files in the folder `./data/str(num_client)`. The data file `new-databricks-dolly-15k.json` for generating each client's local dataset is the first version of `databricks-dolly-15k` , which is a corpus of more than 15,000 records with 8 categeries generated by thousands of [Databricks Lab](https://www.databricks.com/learn/labs) employees. Please refer to their official repository [dolly](https://github.com/databrickslabs/dolly) for the latest version of data.

## Federated Finetuning

To fully leverage the computational resources of each participating client, our lightweight Federated Learning framework employs the well-established parameter-efficient method, [LoRA](https://github.com/microsoft/LoRA), for conducting local training. The local training process is built upon the implementations of Hugging Face's [PEFT](https://github.com/huggingface/peft), Tim Dettmers' [bitsandbytes](https://github.com/TimDettmers/bitsandbytes), and the [Alpaca-lora](https://github.com/tloen/alpaca-lora), enabling the training to be completed within hours on a single NVIDIA TITAN RTX.

Example usage:
```bash
python main.py --global_model './models/vicuna-7b-all-v1.1'\
      --data_path  "./data" \
      --output_dir  './vicuna-lora-shepherd-7b/'\
      --num_communication_rounds 10 \
      --num_clients  10 \
      --train_on_inputs \
      --group_by_length
```

Within the `main.py` file, the GeneralClient is a Python class serves as a representation of the local client and encompasses five distinct sections that facilitate local training: "prepare_local_dataset," "build_local_trainer," "initiate_local_training," "train," and "terminate_local_training." Each of these sections is easy to comprehend and can be easily customized by adding your own functions to meet specific requirements.

We can also tweak the hyperparameters:
```bash
python main.py --global_model './models/vicuna-7b-all-v1.1'\
      --data_path  "./data" \
      --output_dir  './vicuna-lora-shepherd-7b/'\
      --num_communication_rounds 50 \
      --num_clients  10 \
      --client_selection_frac 0.2 \
      --local_batch_size  64 \
      --local_num_epochs  10 \
      --local_micro_batch_size 8 \
      --local_learning_rate 0.0003 \
      --lora_r 16 \
      --lora_target_modules='[q_proj,k_proj,v_proj,o_proj]' \
      --train_on_inputs \
      --group_by_length
```

Our framework supports numerous popular LLMs, such as [LLaMA](https://github.com/facebookresearch/llama), [Alpaca](https://github.com/tatsu-lab/stanford_alpaca), [Vicuna](https://vicuna.lmsys.org/), [Baize](https://github.com/project-baize/baize-chatbot), and others. We welcome any pull requests that adapt our code to support additional models or datasets.

## Inference 

The `GlobalModel_generate.py` file streamlines the inference process for the global model by utilizing a Gradio interface. This file loads the foundation model from the Hugging Face Model Hub and obtains the LoRA weights and configurations from the output directory.

```bash
python GlobalModel_generated.py \
      --load_8bit \
      --base_model './models/vicuna-7b-all-v1.1' \
      --lora_weights_path vicuna-lora-shepherd-7b/10/4/adapter_model.bin \
      --lora_config_path vicuna-lora-shepherd-7b/10/
```

Note:
`--lora_weights_path` is /output/path/to/lora_weights
`--lora_config_path` is /output/path/to/lora_config