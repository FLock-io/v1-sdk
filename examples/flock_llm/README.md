# FLockLLM
FLockLLM is a Large Language Model that has been fine-tuned in a web3-based, decentralized manner.

## Preliminaries

**Prepare pre-trained model weights**

1. Download pre-trained model weights (Vicuna) 
   - could be found via https://drive.google.com/file/d/1ehdGXhKGlIOf8c8hit8QjM1JebCmWrai/view?usp=sharing
2. Unzip the file and move the folder `vicuna-7b-all-v1.1` under the `./model` folder.

## Data Preparation (If you want to use your own data)
We prepared the example data for each client (total 4 clients) in the `./data/4` folder. The example dataset is `Dolly` Dataset from `Databricks Lab`.
(The data file `new-databricks-dolly-15k.json` for generating each client's local dataset is the first version of `databricks-dolly-15k` , which is a corpus of more than 15,000 records with 8 categeries generated by thousands of [Databricks Lab](https://www.databricks.com/learn/labs) employees. Please refer to their official repository [dolly](https://github.com/databrickslabs/dolly) for the latest version of data.)

**To prepare dataset for all clients, run the following command:**
```bash
num_client=10 # The number of clients
diff_quantity=0 # Whether clients have different amounts of data
python client_data_allocation.py $num_client $diff_quantity
```
Running this command will save the data files in the folder `./data/str(num_client)`. 

## Federated Large Language Model Finetuning via FLock Framework
After you have prepared the pretrained model weights and data, you can start the FLockLLM finetuning process.
**Directly run the following command:**
```bash
# Automatically wrapup all source code and materials into a docker image and upload to IPFS server
./build_and_upload.sh
```
After operate the above command, you will get a IPFS Hash code that will be used in the client side, please jump to [FLock client](https://github.com/FLock-io/client).

## Hosting model

The `global_model_host.py` file streamlines the inference workflow of the global model using a Gradio interface. It loads the foundation model from local (also support load from Hugging Face Model Hub) and retrieves LoRA weights and configurations from the output directory

```bash
python global_model_host.py \
      --load_8bit \
      --base_model './model/vicuna-7b-all-v1.1' \
      --lora_weights_path vicuna-lora-shepherd-7b/4/pytorch_local_model_lora.bin \
      --lora_config_path vicuna-lora-shepherd-7b/
      
```
Note:
`--lora_weights_path` is /output/path/to/lora_weights
`--lora_config_path` is /output/path/to/lora_config